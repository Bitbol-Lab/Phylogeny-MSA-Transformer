{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Protein language models trained on multiple sequence alignments learn phylogenetic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "import string\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm\n",
    "\n",
    "from patsy import dmatrices\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "import esm\n",
    "import torch\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting settings\n",
    "SMALL_SIZE = 50\n",
    "MEDIUM_SIZE = 60\n",
    "BIGGER_SIZE = 70\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"times\"],\n",
    "    \"font.size\": MEDIUM_SIZE,\n",
    "    \"axes.titlesize\": BIGGER_SIZE,\n",
    "    \"axes.labelsize\": BIGGER_SIZE,\n",
    "    \"figure.titlesize\": BIGGER_SIZE,\n",
    "    \"xtick.labelsize\": MEDIUM_SIZE,\n",
    "    \"ytick.labelsize\": MEDIUM_SIZE,\n",
    "    \"legend.fontsize\": MEDIUM_SIZE,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities from https://github.com/facebookresearch/esm\n",
    "# This is an efficient way to delete lowercase characters and insertion characters from a string\n",
    "deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "deletekeys[\".\"] = None\n",
    "deletekeys[\"*\"] = None\n",
    "translation = str.maketrans(deletekeys)\n",
    "\n",
    "\n",
    "def remove_insertions(sequence: str) -> str:\n",
    "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "    return sequence.translate(translation)\n",
    "\n",
    "\n",
    "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"    \n",
    "    return [(record.description, remove_insertions(str(record.seq)))\n",
    "            for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msas_folder = pathlib.Path(\"data/Pfam_seed/msa\")\n",
    "\n",
    "pfam_families = [\n",
    "    \"PF00004\",\n",
    "    \"PF00005\",\n",
    "    \"PF00041\",\n",
    "    \"PF00072\",\n",
    "    \"PF00076\",\n",
    "    \"PF00096\",\n",
    "    \"PF00153\",\n",
    "    \"PF00271\",\n",
    "    \"PF00397\",\n",
    "    \"PF00512\",\n",
    "    \"PF00595\",\n",
    "    \"PF01535\",\n",
    "    \"PF02518\",\n",
    "    \"PF07679\",\n",
    "    \"PF13354\"\n",
    "]\n",
    "\n",
    "MAX_DEPTH = 500\n",
    "\n",
    "n_layers = n_heads = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset of Hamming distance matrices and averages of MSA Transformer column attentions averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to host Hamming distance matrices and averaged column attention matrices as .npy files\n",
    "DISTS_FOLDER = pathlib.Path(\"data/hamming\")\n",
    "ATTNS_FOLDER = pathlib.Path(\"data/col_attentions\")\n",
    "\n",
    "for folder in [DISTS_FOLDER, ATTNS_FOLDER]:\n",
    "    if not folder.exists():\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_transformer = msa_transformer.eval()\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()\n",
    "\n",
    "for pfam_family in tqdm.tqdm(pfam_families):\n",
    "    dists_path = DISTS_FOLDER / f\"{pfam_family}_seed.npy\"\n",
    "    attns_path = ATTNS_FOLDER / f\"{pfam_family}_seed_mean-on-cols_symm.npy\"\n",
    "\n",
    "    msa_data = [read_msa(msas_folder / f\"{pfam_family}_seed_hmmalign_no_inserts.fasta\", MAX_DEPTH)]\n",
    "    msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(msa_data)\n",
    "\n",
    "    depth = msa_batch_tokens.shape[1]\n",
    "    with torch.no_grad():\n",
    "        results = msa_transformer(msa_batch_tokens, repr_layers=[12], need_head_weights=True)\n",
    "\n",
    "    # Compute and save averaged and symmetrized column attention matrices\n",
    "    attns_mean_on_cols_symm = results[\"col_attentions\"].cpu().numpy()[0, ...].mean(axis=2)\n",
    "    attns_mean_on_cols_symm += attns_mean_on_cols_symm.transpose(0, 1, 3, 2)\n",
    "    np.save(attns_path, attns_mean_on_cols_symm)\n",
    "\n",
    "    # Compute and save Hamming distance matrices\n",
    "    np.save(dists_path, squareform(pdist(msa_batch_tokens.cpu().numpy()[0, :, 1:], \"hamming\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit one logistic model per MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_train_test_sets(attns,\n",
    "                           dists,\n",
    "                           normalize_dists=False,\n",
    "                           train_size=0.7,\n",
    "                           ensure_same_size=False,\n",
    "                           zero_attention_diagonal=False):\n",
    "    \"\"\"Attentions assumed averaged across column dimensions, i.e. 4D tensors\"\"\"\n",
    "    if zero_attention_diagonal:\n",
    "        attns[:, :, np.arange(attns.shape[2]), np.arange(attns.shape[2])] = 0\n",
    "    assert attns.shape[2] == attns.shape[3]\n",
    "    if normalize_dists:\n",
    "        dists = dists.astype(np.float64)\n",
    "        dists /= np.max(dists)\n",
    "    if ensure_same_size:\n",
    "        dists = dists[:attns.shape[2], :attns.shape[2]]\n",
    "    assert len(dists) == attns.shape[2]\n",
    "    depth = len(dists)\n",
    "    n_layers, n_heads, depth, _ = attns.shape\n",
    "\n",
    "    # Train-test split\n",
    "    n_train = int(train_size * depth)\n",
    "    train_idxs = rng.choice(depth, size=n_train, replace=False)\n",
    "    split_mask = np.zeros(depth, dtype=bool)\n",
    "    split_mask[train_idxs] = True\n",
    "\n",
    "    attns_train, attns_test = attns[:, :, split_mask, :][:, :, :, split_mask], attns[:, :, ~split_mask, :][:, :, :, ~split_mask]\n",
    "    dists_train, dists_test = dists[split_mask, :][:, split_mask], dists[~split_mask, :][:, ~split_mask]\n",
    "    \n",
    "    n_rows_train, n_rows_test = attns_train.shape[-1], attns_test.shape[-1]\n",
    "    triu_indices_train = np.triu_indices(n_rows_train)\n",
    "    triu_indices_test = np.triu_indices(n_rows_test)\n",
    "    \n",
    "    attns_train = attns_train[..., triu_indices_train[0], triu_indices_train[1]]\n",
    "    attns_test = attns_test[..., triu_indices_test[0], triu_indices_test[1]]\n",
    "    dists_train = dists_train[triu_indices_train]\n",
    "    dists_test = dists_test[triu_indices_test]\n",
    "    \n",
    "    attns_train = attns_train.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
    "    attns_test = attns_test.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
    "\n",
    "    return (attns_train, dists_train), (attns_test, dists_test), (n_rows_train, n_rows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_regressions_msawise(normalize_dists=False,\n",
    "                                ensure_same_size=False,\n",
    "                                zero_attention_diagonal=False):\n",
    "    regr_results = {}\n",
    "    for pfam_family in tqdm.tqdm(pfam_families):\n",
    "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_seed.npy\")\n",
    "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_seed_mean-on-cols_symm.npy\")\n",
    "\n",
    "        ((attns_train, dists_train),\n",
    "         (attns_test, dists_test),\n",
    "         (n_rows_train, n_rows_test)) = create_train_test_sets(attns,\n",
    "                                                               dists,\n",
    "                                                               normalize_dists=normalize_dists,\n",
    "                                                               ensure_same_size=ensure_same_size,\n",
    "                                                               zero_attention_diagonal=zero_attention_diagonal)\n",
    "\n",
    "        df_train = pd.DataFrame(attns_train,\n",
    "                                columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
    "        df_train[\"dist\"] = dists_train\n",
    "        df_test = pd.DataFrame(attns_test,\n",
    "                               columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
    "        df_test[\"dist\"] = dists_test\n",
    "\n",
    "        # Carve out the training matrices from the training and testing data frame using the regression formula\n",
    "        formula = \"dist ~ \" + \" + \".join([f'lyr{i}_hd{j}' for i in range(n_layers) for j in range(n_heads)])\n",
    "        y_train, X_train = dmatrices(formula, df_train, return_type=\"dataframe\")\n",
    "        y_test, X_test = dmatrices(formula, df_test, return_type=\"dataframe\")\n",
    "\n",
    "        binom_model = sm.GLM(y_train, X_train, family=sm.families.Binomial(), cov_type=\"H0\")\n",
    "        binom_model_results = binom_model.fit(maxiter=200, tol=1e-9)\n",
    "\n",
    "        y_train = y_train[\"dist\"].to_numpy()\n",
    "        y_test = y_test[\"dist\"].to_numpy()\n",
    "        y_pred_train = binom_model_results.predict(X_train).to_numpy()\n",
    "        y_pred_test = binom_model_results.predict(X_test).to_numpy()\n",
    "\n",
    "        regr_results[pfam_family] = {\n",
    "            \"bias\": binom_model_results.params[0],\n",
    "            \"coeffs\": binom_model_results.params.to_numpy()[-n_layers * n_heads:].reshape(n_layers, n_heads),\n",
    "            \"y_train\": y_train,\n",
    "            \"y_pred_train\": y_pred_train,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred_test\": y_pred_test,\n",
    "            \"depth\": dists.shape[0],\n",
    "            \"n_rows_train\": n_rows_train,\n",
    "            \"n_rows_test\": n_rows_test\n",
    "            }\n",
    "    \n",
    "    return regr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_results_hamming_msawise = perform_regressions_msawise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_comparison_mat(y, y_pred, n_rows):\n",
    "    assert len(y) == len(y_pred)\n",
    "\n",
    "    comparison_mat = np.zeros((n_rows, n_rows), dtype=np.float32)\n",
    "    ct = 0\n",
    "    for i in range(n_rows):\n",
    "        for j in range(i, n_rows):\n",
    "            # Order is important as we want the diagonal to be a prediction\n",
    "            comparison_mat[i, j] = y[ct]\n",
    "            comparison_mat[j, i] = y_pred[ct]\n",
    "            ct += 1\n",
    "    assert ct == len(y)\n",
    "    \n",
    "    return comparison_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only Pfam families with depth >= 200 and length >=50\n",
    "pfam_families_selec = [\"PF00004\", \"PF00271\", \"PF00512\", \"PF02518\"]\n",
    "n_selec = len(pfam_families_selec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.bwr\n",
    "vpad = 30\n",
    "x_vals_coeffs = np.arange(0, n_heads, 2)\n",
    "y_vals_coeffs = np.arange(0, n_layers, 2)\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    figsize=(40, 10 * n_selec),\n",
    "    nrows=n_selec,\n",
    "    ncols=4,\n",
    "    gridspec_kw={\"width_ratios\": [10, 3, 10, 10]},\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for i, pfam_family in enumerate(pfam_families_selec):\n",
    "    res = regr_results_hamming_msawise[pfam_family]\n",
    "    for key in res:\n",
    "        exec(f\"{key} = res['{key}']\")\n",
    "\n",
    "    im = axs[i, 0].imshow(coeffs, norm=colors.CenteredNorm(), cmap=cmap)\n",
    "    cbar = fig.colorbar(im, ax=axs[i, 0], fraction=0.05, pad=0.03)\n",
    "    axs[i, 0].set_ylabel(fr\"\\bf {pfam_family}\" + \"\\nLayer\")\n",
    "    axs[i, 0].set_xticks(x_vals_coeffs)\n",
    "    axs[i, 0].set_yticks(y_vals_coeffs)\n",
    "    axs[i, 0].set_xticklabels(list(map(str, x_vals_coeffs + 1)))\n",
    "    axs[i, 0].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
    "\n",
    "    axs[i, 1].plot(np.mean(np.abs(coeffs), axis=1),\n",
    "                   np.arange(n_layers),\n",
    "                   \"-o\",\n",
    "                   markersize=12,\n",
    "                   lw=5)\n",
    "    axs[i, 1].invert_yaxis()\n",
    "    axs[i, 1].set_yticks(y_vals_coeffs)\n",
    "    axs[i, 1].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
    "    axs[i, 1].set_xlim([0, 55])\n",
    "    axs[i, 1].set_ylabel(\"Layer\")\n",
    "\n",
    "    for j, y, y_pred, n_rows in [(2, y_train, y_pred_train, n_rows_train),\n",
    "                                 (3, y_test, y_pred_test, n_rows_test)]:\n",
    "        # 2 is train, 3 is test\n",
    "        hamming_comparison = create_dist_comparison_mat(y, y_pred, n_rows)\n",
    "        axs[i, j].imshow(np.triu(hamming_comparison, k=1) + np.tril(np.full_like(hamming_comparison, fill_value=np.nan)),\n",
    "                         cmap=\"Blues\",\n",
    "                         vmin=0,\n",
    "                         vmax=1)\n",
    "        pos = axs[i, j].imshow(np.tril(hamming_comparison) + np.triu(np.full_like(hamming_comparison, fill_value=np.nan), k=1),\n",
    "                               cmap=\"Greens\",\n",
    "                               vmin=0,\n",
    "                               vmax=1)\n",
    "    \n",
    "    axs[i, 2].set_ylabel(\"Sequence\")\n",
    "\n",
    "axs[0, 0].set_title(\"Regression coefficients\", pad=vpad)\n",
    "axs[0, 1].set_title(\"Avg.\\ abs.\\ coeff.\", pad=vpad)\n",
    "axs[0, 2].set_title(\"Training\", pad=vpad)\n",
    "axs[0, 3].set_title(\"Test\", pad=vpad)\n",
    "\n",
    "axs[-1, 0].set_xlabel(\"Head\")\n",
    "axs[-1, 2].set_xlabel(\"Sequence\")\n",
    "axs[-1, 3].set_xlabel(\"Sequence\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regr_results_hamming_msawise = pd.DataFrame()\n",
    "for pfam_family in pfam_families:\n",
    "    res = regr_results_hamming_msawise[pfam_family]\n",
    "    for key in res:\n",
    "        exec(f\"{key} = res['{key}']\")\n",
    "    n_samples_train = len(y_train)\n",
    "    n_samples_test = len(y_test)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"Depth\"] = depth\n",
    "    \n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"mean (training)\"] = np.mean(y_train)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"mean (test)\"] = np.mean(y_test)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"std (training)\"] = np.std(y_train)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"std (test)\"] = np.std(y_test)\n",
    "    \n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"RMSE (training)\"] = np.linalg.norm(y_train - y_pred_train) / np.sqrt(n_samples_train)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"RMSE (test)\"] = np.linalg.norm(y_test - y_pred_test) / np.sqrt(n_samples_test)\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"MAE (training)\"] = np.sum(np.abs(y_train - y_pred_train)) / n_samples_train\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"MAE (test)\"] = np.sum(np.abs(y_test - y_pred_test)) / n_samples_test\n",
    "    \n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"R^2 (test)\"] = 1 - np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "    pearson = pearsonr(y_test, y_pred_test)[0]\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"Pearson (test)\"] = pearson\n",
    "    df_regr_results_hamming_msawise.loc[pfam_family, \"Slope (test)\"] = pearson * np.std(y_pred_test) / np.std(y_test)\n",
    "\n",
    "df_regr_results_hamming_msawise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression coefficients from different MSAs are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pearsons_coeffs = []\n",
    "for i, pfam_family_x in enumerate(pfam_families):\n",
    "    for pfam_family_y in pfam_families[i + 1:]:\n",
    "        x = regr_results_hamming_msawise[pfam_family_x][\"coeffs\"].flatten()\n",
    "        y = regr_results_hamming_msawise[pfam_family_y][\"coeffs\"].flatten()\n",
    "        pearsons_coeffs.append(pearsonr(x, y)[0])\n",
    "pearsons_coeffs = squareform(np.array(pearsons_coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only Pfam families with depth >= 100 and length >=30\n",
    "pfam_families_selec = [\"PF00004\", \"PF00153\", \"PF00271\", \"PF00397\", \"PF00512\", \"PF01535\", \"PF02518\"]\n",
    "mask = np.isin(np.array(pfam_families), [pfam_families_selec])\n",
    "pearsons_coeffs_selec = pearsons_coeffs[mask, :][:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [pfam_family for i, pfam_family in enumerate(pfam_families) if mask[i]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12),\n",
    "                       constrained_layout=True)\n",
    "im = ax.imshow(np.tril(pearsons_coeffs_selec),\n",
    "               cmap=\"Blues\",\n",
    "               aspect=\"equal\",\n",
    "               vmin=0, vmax=1)\n",
    "ax.set_yticks(np.arange(len(pearsons_coeffs_selec)),\n",
    "              labels=labels)\n",
    "ax.set_xticks(np.arange(len(pearsons_coeffs_selec)),\n",
    "              labels=labels,\n",
    "              rotation=45,\n",
    "              ha=\"right\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.05, pad=0.04, label=\"Pearson correlation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit one common logistic model across MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_families_train = pfam_families[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_regressions_msawise(pfam_families_train):\n",
    "    df = pd.DataFrame(columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)] + [\"dist\"], dtype=np.float64)\n",
    "\n",
    "    for pfam_family in pfam_families_train:\n",
    "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_seed.npy\")\n",
    "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_seed_mean-on-cols_symm.npy\")\n",
    "\n",
    "        triu_indices = np.triu_indices(attns.shape[-1])\n",
    "        attns = attns[..., triu_indices[0], triu_indices[1]]\n",
    "        dists = dists[triu_indices]\n",
    "        df2 = pd.DataFrame(attns.transpose(2, 0, 1).reshape(-1, n_layers * n_heads),\n",
    "                           columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_layers)])\n",
    "        df2[\"dist\"] = dists\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "    # Carve out the training matrices from the training and testing data frame using the regression formula\n",
    "    formula = \"dist ~ \" + \" + \".join([f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
    "    y_train, X_train = dmatrices(formula, df, return_type=\"dataframe\")\n",
    "\n",
    "    # Fit the model\n",
    "    binom_model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "    binom_model_results = binom_model.fit(maxiter=200, tol=1e-9)\n",
    "\n",
    "    regr_results_hamming_common = {}\n",
    "    for pfam_family in pfam_families:\n",
    "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_seed.npy\")\n",
    "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_seed_mean-on-cols_symm.npy\")\n",
    "        depth = len(dists)\n",
    "\n",
    "        triu_indices = np.triu_indices(depth)\n",
    "        attns = attns[..., triu_indices[0], triu_indices[1]]\n",
    "        dists = dists[triu_indices]\n",
    "        attns = attns.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
    "\n",
    "        df = pd.DataFrame(attns,\n",
    "                          columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
    "        df[\"dist\"] = dists\n",
    "        _, X = dmatrices(formula, df, return_type=\"dataframe\")\n",
    "\n",
    "        y_pred = binom_model_results.predict(X).to_numpy()\n",
    "\n",
    "        regr_results_hamming_common[pfam_family] = {\n",
    "            \"bias\": binom_model_results.params[0],\n",
    "            \"coeffs\": binom_model_results.params.to_numpy()[-n_layers * n_heads:].reshape(n_layers, n_heads),\n",
    "            \"y\": dists,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"depth\": depth,\n",
    "        }\n",
    "\n",
    "    return regr_results_hamming_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_results_hamming_common = perform_regressions_msawise(pfam_families_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(43, 10),\n",
    "                        nrows=1,\n",
    "                        ncols=5,\n",
    "                        gridspec_kw={\"width_ratios\": [10, 3, 10, 10, 10]},\n",
    "                        constrained_layout=True)\n",
    "\n",
    "coeffs = regr_results_hamming_common[pfam_families[0]][\"coeffs\"]\n",
    "im = axs[0].imshow(coeffs, norm=colors.CenteredNorm(), cmap=cmap)\n",
    "cbar = fig.colorbar(im, ax=axs[0], fraction=0.05, pad=0.03)\n",
    "axs[0].set_xticks(x_vals_coeffs)\n",
    "axs[0].set_yticks(y_vals_coeffs)\n",
    "axs[0].set_xticklabels(list(map(str, x_vals_coeffs + 1)))\n",
    "axs[0].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
    "\n",
    "axs[1].plot(np.mean(np.abs(coeffs), axis=1),\n",
    "            np.arange(n_layers),\n",
    "            \"-o\",\n",
    "            markersize=12,\n",
    "            lw=5)\n",
    "axs[1].invert_yaxis()\n",
    "axs[1].set_yticks(y_vals_coeffs)\n",
    "axs[1].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
    "axs[1].set_xticks([0, 10, 20])\n",
    "\n",
    "axs[0].set_title(\"Regression coefficients\", pad=vpad)\n",
    "axs[1].set_title(\"Avg.\\ abs.\\ coeff.\", pad=vpad)\n",
    "\n",
    "for i, pfam_family in enumerate(pfam_families[-3:]):\n",
    "    y = regr_results_hamming_common[pfam_family][\"y\"]\n",
    "    y_pred = regr_results_hamming_common[pfam_family][\"y_pred\"]\n",
    "    n_rows = regr_results_hamming_common[pfam_family][\"depth\"]\n",
    "\n",
    "    hamming_comparison = create_dist_comparison_mat(y, y_pred, n_rows)\n",
    "    axs[2 + i].imshow(np.triu(hamming_comparison, k=1) + np.tril(np.full_like(hamming_comparison, fill_value=np.nan)),\n",
    "                      cmap=\"Blues\",\n",
    "                      vmin=0,\n",
    "                      vmax=1)\n",
    "    pos = axs[2 + i].imshow(np.tril(hamming_comparison) + np.triu(np.full_like(hamming_comparison, fill_value=np.nan), k=1),\n",
    "                            cmap=\"Greens\",\n",
    "                            vmin=0,\n",
    "                            vmax=1)\n",
    "\n",
    "    axs[2 + i].set_xlabel(\"Sequence\")\n",
    "\n",
    "    axs[2 + i].set_title(pfam_family, pad=vpad)\n",
    "\n",
    "axs[0].set_ylabel(\"Layer\")\n",
    "axs[0].set_xlabel(\"Head\")\n",
    "axs[1].set_ylabel(\"Layer\")\n",
    "axs[2].set_ylabel(\"Sequence\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regr_results_hamming_common = pd.DataFrame()\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(25, 15),\n",
    "                        nrows=3,\n",
    "                        ncols=5,\n",
    "                        sharex=True,\n",
    "                        sharey=True,\n",
    "                        constrained_layout=True)\n",
    "\n",
    "for i, pfam_family in enumerate(pfam_families):\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"Depth\"] = regr_results_hamming_common[pfam_family][\"depth\"]\n",
    "    y = regr_results_hamming_common[pfam_family][\"y\"]\n",
    "    y_pred = regr_results_hamming_common[pfam_family][\"y_pred\"]\n",
    "    n_samples = len(y)\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"RMSE\"] = np.linalg.norm(y - y_pred) / np.sqrt(n_samples)\n",
    "    y_std = np.std(y)\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"Std\"] = y_std\n",
    "    pearson = pearsonr(y, y_pred)[0]\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"Pearson\"] = pearson\n",
    "    slope = pearson * y_std / np.std(y_pred)\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"Slope\"] = slope\n",
    "    df_regr_results_hamming_common.loc[pfam_family, \"R^2\"] = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "    intercept = np.mean(y) - slope * np.mean(y_pred)\n",
    "    \n",
    "    axs.flat[i].set_title(pfam_family, fontsize=30)\n",
    "    axs.flat[i].scatter(y_pred, y, s=1)\n",
    "    axs.flat[i].axline((0, intercept), slope=slope, linewidth=2, color='r')\n",
    "\n",
    "    plt.setp(axs.flat[i].get_yticklabels(), fontsize=20)\n",
    "    plt.setp(axs.flat[i].get_xticklabels(), fontsize=20)\n",
    "\n",
    "fig.supxlabel(\"Predicted\", fontsize=40)\n",
    "fig.supylabel(\"Actual\", fontsize=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regr_results_hamming_common.sort_values(\"Depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training + test MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(25, 15),\n",
    "                        nrows=3,\n",
    "                        ncols=5,\n",
    "                        constrained_layout=True)\n",
    "\n",
    "for i, pfam_family in enumerate(pfam_families):\n",
    "    y = regr_results_hamming_common[pfam_family][\"y\"]\n",
    "    y_pred = regr_results_hamming_common[pfam_family][\"y_pred\"]\n",
    "    n_rows = regr_results_hamming_common[pfam_family][\"depth\"]\n",
    "    \n",
    "    hamming_comparison = create_dist_comparison_mat(y, y_pred, n_rows)\n",
    "    axs.flat[i].imshow(np.triu(hamming_comparison, k=1) + np.tril(np.full_like(hamming_comparison, fill_value=np.nan)),\n",
    "                       cmap=\"Blues\",\n",
    "                       vmin=0,\n",
    "                       vmax=1)\n",
    "    axs.flat[i].imshow(np.tril(hamming_comparison) + np.triu(np.full_like(hamming_comparison, fill_value=np.nan), k=1),\n",
    "                       cmap=\"Greens\",\n",
    "                       vmin=0,\n",
    "                       vmax=1)\n",
    "    \n",
    "    axs.flat[i].set_title(pfam_family, fontsize=30)\n",
    "\n",
    "    plt.setp(axs.flat[i].get_yticklabels(), fontsize=20)\n",
    "    plt.setp(axs.flat[i].get_xticklabels(), fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
